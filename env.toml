## Global variables.
[benchmark]
namespace = "benchmark-$$username()"
trace_command = false
show_command_output = false
log_level = "info"
# system possible values are risingwave, flink.
system = "risingwave"

[trick.symbols]
empty = ""

# Options are deprecated. Will be removed soon.
[benchmark.cli]
helm = { update = false }

[benchmark.logging]
tail = 1000

## AWS cli related variables.
[benchmark.aws]
ask_before_proceed = true

[benchmark.aws.credentials]
access_key = ""
access_secret = ""

## gcloud cli related variables.
[benchmark.gcloud]
ask_before_proceed = true
# gcloud service account key file location: /User/xxx/rwcdev-kubebench.json
service_account_key_file = ""

## Job related variables.
[benchmark.job]
# RisingWave system type possible values are nexmark-kafka.
# Flink system type possible values is flink-nexmark-kafka.
name = "benchmark-job"
type = "nexmark"

[benchmark.job.resources]
cpu = { limit = "4", request = "4" }
mem = { limit = "8Gi", request = "8Gi" }

[benchmark.job.wait]
# Time to wait for prepare and clean job to complete or fail.
# retry_limit * retry_interval = wait_time(s)
retry_limit = 300
retry_interval = 1

## Benchmark nexmark (using nexmark source generator)
[benchmark.nexmark]
# Nexmark queries to run.
# For raw queries, please refer to the manifest templates under ./manifests/nexmark or ./manifests/flink-nexmark.
# Query list is supported. Set multiple queries seperated with comma.
query = "q0,q3"
# STREAMING_PARALLELISM for materialized views. Set to 0 to use default CPU size based parallelism.
streaming_parallelism = 0

[benchmark.nexmark.source]
watermark = false

## Benchmark nexmark (using nexmark Kafka source)
# https://github.com/risingwavelabs/nemxark-bench/
# the events are limited and generated before creating MV, use this when doing performance test
# prepare phase: create topics and generate events to Kafka
# start phase: create MV, and start to ingest data
[benchmark.nexmark_kafka]
image = { tag = "latest" }
image_pull_policy = "IfNotPresent"
log_level = "info"
max_events = 100
generator_thread_num = 4
event_rate = 100
partition = 3
separate_topics = false
watermark = false
# if keep_kafka_data is true. The topics and data in kafka won't be cleaned after `./benchmark clean`, but still be cleaned after`./benchmark teardown`.
keep_kafka_data = false
# This config is used to specify event types to skip to achieve better data-generating rates.
# For example, if you are benchmarking using nexmark q5 and q5 involve only the bid events.
# You can set `skip_event_types = "auction, person"`.
# Then the generator will only generate bid events, and now the `event_rate` is the bid event generate rate.
skip_event_types = ""
# If skip_insert_kafka is true. The prepare job will skip inserting kafka.
skip_insert_kafka = false
# If enable_blackhole is true. The prepare job will use `create sink(blackhole)` instead of `create mv`.
enable_blackhole = false

## RisingWave related variables.
[benchmark.risingwave]
name = "benchmark-risingwave"
# Image tag to be used when provisioning the RisingWave, valid tags can be found here:
# https://github.com/risingwavelabs/risingwave/pkgs/container/risingwave.
version = "v1.0.0"
# Timeout is deprecated and will be removed soon. Use [benchmark.risingwave.wait] instead.
timeout = 300
# Log level of RisingWave processes. Set it to "debug" if you would like to see the debug logs.
log_level = "info"
# The location of query log, which records what queries have been issued by users.
# WARNING: Be care that because the directory isn't in a persistent volume,
# the Pod will be forcefully evicted if there are too many logs and the total size exceeds
# some certain limit (deduced by the kubelet from the host's disk resources).
# And you will find a new Pod recreated on the same host afterwards.
# Enable query log by un-commenting the following line.
# query_log_path = "./"

# Generating stub, no need to touch
[benchmark.risingwave.gen]
node_envs="{}"

[benchmark.pods.distribution]
# Node selectors seperated by comma(,), e.g., node_selectors=node-group:test,label_a:value
node_selectors = ""
# Mutual exclusive policy to define the Pods' anti affinity. The allowed values are:
# - global, means all Pods among all namespaces with the same key won't share a host.
# - namespace, means all Pods in the benchmarking namespace with the same key won't share a host.
# - <empty>, no restrictions.
mutual_exclusive_policy = ""
# Mutual exclusive key to define the Pods' anti affinity. Different keys doesn't affect each other.
mutual_exclusive_key = ""
# This parameter is the affinity setting defined for comparing flink performance
# - true,the compactor and compute pods in the cluster are on a same node
# - false,the compactor and compute pods in the cluster are on different nodes
compactor_compute_affinity_enabled = false

# Generated stub. No need to touch.
[benchmark.pods.distribution.gen]
node_selector = "null"
affinity = "null"
affinity_zk = "null"
affinity_frontend_meta = "null"
affinity_compute = "null"

[benchmark.risingwave.wait]
retry_limit = 60
retry_interval = 5

# Generated stub. No need to touch.
[benchmark.risingwave.storage.gen]
objects = "null"

[benchmark.risingwave.storage]
# Storage type, valid value is minio, s3, gcs, azureblob or s3c(s3 compatible).
type = "s3"

[benchmark.risingwave.storage.s3]
region = ""
access_key = ""
access_secret = ""
bucket = "hummock001"
data_directory = "hummock_001"

# ----------------------------------------------------
# Below options are for s3-compatible backends.
# ----------------------------------------------------
[benchmark.risingwave.storage.s3c]
# Vendor name of the S3 compatible services, e.g., aliyun, tencent-cloud, lyve-cloud.
# Please provide with the name for distinguishing.
vendor = "lyve-cloud"
# Endpoint for S3 compatible services, such as OSS, COS, Seagate. Empty means using the AWS S3. Note there are two variables
# you can refer to in the endpoint,
# - REGION, refer it with \\${REGION}, and the operator will automaticall substitue it into the region defined below,
# - BUCKET, refer it with \\${BUCKET}, and will get the same effect as the region.
endpoint = "s3.\\${REGION}.lyvecloud.seagate.com"
# Enforce the virtual hosted or not. Leave it to false in most cases. Till now, Aliyun OSS is the only vendor that requires it to be true.
# Learn more about it at https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html.
virtual_hosted_style = false
region = "ap-southeast-1"
access_key = ""
access_secret = ""
bucket = "hummock001"
data_directory = "hummock_001"

[benchmark.s3.bucket]
create_enabled = false
delete_enabled = false

[benchmark.risingwave.storage.gcs]
location = "asia-southeast1"
bucket = "hummock001"
data_directory = "hummock_001"
# Generated stub, no need to touch.
# Just provide BENCHMARK_GCLOUD_SERVICE_ACCOUNT_KEY_FILE
service_account_credentials = "null"

[benchmark.gcs.bucket]
create_enabled = false
delete_enabled = false

[benchmark.risingwave.storage.azureblob]
endpoint = "https://xxx.blob.core.windows.net/"
container = ""
account_name = ""
account_key = ""
data_directory = "hummock_001"

[benchmark.risingwave.replicas]
meta = 1
frontend = 1
compute = 1
compactor = 1
connector = 1

[benchmark.risingwave.resources]
# Both request and limit should be provided. Make sure request <= limit.
# Examples:
#   cpu (number): 1, 2, 3, 4
#   cpu (milli): 100m (=0.1), 500m (=0.5)
#   memory (SI): 100Mi, 500Mi, 1Gi, 4Gi
cpu = { limit = "4", request = "4" }
mem = { limit = "16Gi", request = "16Gi" }

# When compactor_compute_affinity_enabled = true,
# the following compactor and compute resource configurations will take effect,
# it should allocate the node's mem to compactor and compute on demand.
[benchmark.risingwave.resources.compute]
cpu = { limit = "4", request = "0" }
mem = { limit = "12Gi", request = "0Gi" }

[benchmark.risingwave.resources.compactor]
cpu = { limit = "4", request = "0" }
mem = { limit = "4Gi", request = "0Gi" }

[benchmark.risingwave.meta.memory_profiling]
# Enable profile or not.
enable = false
# Log-scale profiling interval in memory allocation, e.g., 40 = (1 << 40) bytes = 1TB
lg_prof_interval = 32
# Log-scale profiling sample interval in memory allocation, e.g., 19 = (1 << 19) bytes = 512K
lg_prof_sample = 19

[benchmark.risingwave.compactor.memory_profiling]
# Enable profile or not.
enable = false
# Log-scale profiling interval in memory allocation, e.g., 40 = (1 << 40) bytes = 1TB
lg_prof_interval = 38
# Log-scale profiling sample interval in memory allocation, e.g., 19 = (1 << 19) bytes = 512K
lg_prof_sample = 19

[benchmark.risingwave.compute.memory_profiling]
# Enable profile or not.
enable = false
# Log-scale profiling interval in memory allocation, e.g., 40 = (1 << 40) bytes = 1TB
lg_prof_interval = 40
# Log-scale profiling sample interval in memory allocation, e.g., 19 = (1 << 19) bytes = 512K
lg_prof_sample = 19

[benchmark.risingwave.configs]
telemetry_enabled = false
### If dir is empty, then file cache is disabled
file_cache_dir = ""

## Kafka related variables.
[benchmark.kafka]
enabled = false
name = "benchmark-kafka"
log_retention_bytes = "-1"
log_retention_hours = -1
# If keep_pod_enabled = true, it will keep kafka pod when teardown cluster.
keep_pod_enabled = false

[benchmark.kafka.resources]
cpu = { limit = "4", request = "4" }
mem = { limit = "16Gi", request = "16Gi" }

[benchmark.kafka.persistence]
enabled = true
storage_class = ""
size = "8Gi"

[benchmark.kafka.zookeeper.persistence]
enabled = true
storage_class = ""

[benchmark.kafka.metrics]
kafka_enabled = false
jmx_enabled = false
servicemonitor_enabled = false

## Etcd related variables.
[benchmark.etcd]
name = "benchmark-etcd"

[benchmark.etcd.resources]
cpu = { limit = "4", request = "4" }
mem = { limit = "16Gi", request = "16Gi" }

[benchmark.etcd.persistence]
enabled = true
storage_class = ""

## Flink related variables.
[benchmark.flink]
enabled = false
name = "benchmark-flink"
taskmanager_replicas = 1
taskmanager_slots = 8
parallelism = 8
jobmanager_memory = "16G"
taskmanager_memory = "16G"
# 60 seconds
checkpoint_interval = 60000
image = { tag = "v1.16.0" }
### about state or rocksdb
### We refer to https://developer.aliyun.com/article/772873 as a better configuration
# Flink default is true.
# Only when this variable is false, we can set some other variables such as
# state.backend.rocksdb.block.cache-size (corresponding to block_cache_size in RocksDB),
# state.backend.rocksdb.writebuffer.size (corresponding to write_buffer_size in RocksDB),
# state.backend.rocksdb.writebuffer.count.
# Please refer to https://flink.apache.org/2021/01/18/using-rocksdb-state-backend-in-apache-flink-when-and-how/#tuning-rocksdb
# We use the managed mode by default as these configurations are hard to adjust and very likely to be
# highly workload-dependent.
# If we use managed memory, then we can choose to specify either
# taskmanager.memory.managed.fraction or taskmanager.memory.managed.size but not both.
# Flink default is 0.4
taskmanager_memory_managed_fraction = 0.4
# Flink default is 4kb
state_backend_rocksdb_block_blocksize = "4KB"
# Flink default is 2
state_backend_rocksdb_thread_num = 2
# Flink default is false
state_backend_rocksdb_compaction_level_use_dynamic_size = false
# Flink default is 64MB
state_backend_rocksdb_compaction_level_target_file_size_base = "64MB"
# Flink default is 2
state_backend_rocksdb_writebuffer_count = 2
# Flink default is 1
state_backend_rocksdb_writebuffer_number_to_merge = 1
# Flink default is false
pipeline_object_reuse = false
# Flink default is 1
execution_checkpointing_max_concurrent_checkpoints = 1
# Flink default is true
execution_checkpointing_checkpoints_after_tasks_finish_enabled = true
# Flink default is false.
state_backend_rocksdb_use_bloom_filter = false
# Flink default is false. According to https://github.com/aayushKumarJarvis/rocks-wiki/blob/master/RocksDB-Bloom-Filter.md#new-bloom-filter-format, full filter improves read performance at the cost of memory. As 1B nexmark events generally do not take too much space for states, therefore we choose the default option, i.e. using full filter
# state_backend_rocksdb_bloom_filter_block_based_mode = false
# Flink default is false
state_backend_incremental = true
# configuration options for adjusting and tuning table programs
# Didn't find any of the following table configurations at https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/, but they show up at https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/table/tuning/
# When `mini batch` is enabled, Flink may OOM as the memory usage of `mini batch` is not under Flink's control. Therefore, we disable them by default.
table_exec_mini_batch_enabled = true
table_exec_mini_batch_allow_latency = "2s"
table_exec_mini_batch_size = 50000
# Without `mini batch` enabled, the following config does not make sense as the extra local aggregation will only slow down the performance.
table_optimizer_distinct_agg_split_enabled = true
# JVM options for GC
ENV_JAVA_OPTS = "-verbose:gc -XX:NewRatio=3 -XX:+PrintGCDetails -Xlog:gc* -XX:ParallelGCThreads=4"
#ENV_JAVA_OPTS_JOBMANAGER = "-Xloggc:$FLINK_LOG_DIR/jobmanager-gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=512M"
#ENV_JAVA_OPTS_TASKMANAGER = "-Xloggc:$FLINK_LOG_DIR/taskmanager-gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=512M"
# restart strategy
RESTART_STRATEGY = "fixed-delay"
RESTART_STRATEGY_FIXED_DELAY_ATTEMPTS = 2147483647
RESTART_STRATEGY_FIXED_DELAY_DELAY = "10s"
# job manager
JOBMANAGER_EXECUTION_ATTEMPTS_HISTORY_SIZE = 100

# Number of extra network buffers to use for each outgoing/incoming gate
# (result partition/input gate).
# Flink default is 8
taskmanager_network_memory_floating_buffers_per_gate = 256
# Flink default is 10000
taskmanager_network_request_backoff_max = 30000
# The number of buffers available for each external blocking channel. Cannot find in flink's documentation
TASKMANAGER_NETWORK_MEMORY_BUFFERS_PER_EXTERNAL_BLOCKING_CHANNEL = 16
# The maximum number of concurrent requests in the reduce-side tasks.
TASK_EXTERNAL_SHUFFLE_MAX_CONCURRENT_REQUESTS = 512
# Whether to enable compress shuffle data when using external shuffle.
TASK_EXTERNAL_SHUFFLE_COMPRESSION_ENABLE = true

# The flink s3 bucket is used to store the checkpoints of the rocksdb state backend.
# The bucket_folder indicates the checkpoint directory of each namespace, which can be specified
[benchmark.flink.s3]
access_key = ""
access_secret = ""
bucket = "flink-bench-checkpoints"
bucket_folder = "flink-bench-folder"

[benchmark.flink.resources]
cpu = { limit = "8", request = "8" }
mem = { limit = "16Gi", request = "16Gi" }

[benchmark.flink.persistence]
# Please use the types that exist in the k8s environment
storage_class = "standard"
size = "100Gi"
